{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ff08c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ddbe7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation=None):\n",
    "        \"\"\"\n",
    "          Initialize a neuron with weights and bias\n",
    "\n",
    "          Args:\n",
    "              weights: Weight vector for the neuron\n",
    "              bias: Bias value for the neuron\n",
    "              activation: Activation function for the neuron\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def compute(self, inputs):\n",
    "      \"\"\"\n",
    "        Compute neuron output for given inputs\n",
    "\n",
    "        Args:\n",
    "            inputs: Input vector\n",
    "\n",
    "        Returns:\n",
    "            Neuron output after applying activation function\n",
    "      \"\"\"\n",
    "      # Vectorized weighted sum\n",
    "      z = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "      # Apply activation function if provided\n",
    "      if self.activation is not None:\n",
    "          z = self.activation(z)\n",
    "      return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5bec72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Predefined activations\n",
    "def get_activation(name):\n",
    "  if name == 'tanh':\n",
    "    return np.tanh, lambda x: 1 - np.tanh(x)**2\n",
    "\n",
    "  elif name == 'sigmoid':\n",
    "    sig = lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    return sig, lambda x: sig(x) * (1 - sig(x))\n",
    "\n",
    "  elif name == 'relu':\n",
    "    return lambda x: np.maximum(0, x), lambda x: (x > 0).astype(float)\n",
    "\n",
    "  elif name == 'softmax':\n",
    "        def softmax(x):\n",
    "            # Subtract max for numerical stability\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        def softmax_derivative(x):\n",
    "            return np.ones_like(x)\n",
    "        return softmax, softmax_derivative\n",
    "\n",
    "  elif name is None or name =='linear':\n",
    "    return None, lambda x: np.ones_like(x)\n",
    "    \n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported activation function: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae06e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self, num_neurons, num_inputs, activation=None):\n",
    "      \"\"\"\n",
    "        Initialize a layer with multiple neurons\n",
    "\n",
    "        Args:\n",
    "            num_neurons: Number of neurons in the layer\n",
    "            num_inputs: Number of inputs to each neuron\n",
    "            activation: Activation function for all neurons in the layer\n",
    "        \"\"\"\n",
    "      self.num_neurons = num_neurons\n",
    "      self.num_inputs = num_inputs\n",
    "\n",
    "      # Handle string or (func, deriv) tuple\n",
    "      if isinstance(activation, str):\n",
    "        self.activation, self.activation_derivative = get_activation(activation)\n",
    "      elif isinstance(activation, tuple):\n",
    "        self.activation, self.activation_derivative = activation\n",
    "      elif activation is None:\n",
    "        self.activation, self.activation_derivative = get_activation(activation)\n",
    "      else:\n",
    "        raise ValueError(\"Activation function must be a string, tuple (func, deriv), or None\")\n",
    "\n",
    "      # Initialize weights and biases for all neurons\n",
    "      if self.activation == np.tanh or 'tanh' in str(self.activation):\n",
    "          std = np.sqrt(1 / self.num_inputs)  # Xavier\n",
    "      else:  # ReLU or sigmoid\n",
    "          std = np.sqrt(2 / self.num_inputs)  # He\n",
    "      self.weights = np.random.randn(num_neurons, num_inputs) * std\n",
    "      self.biases = np.zeros(num_neurons)  # Biases to zero is standard\n",
    "\n",
    "      # For backward pass\n",
    "      self.input = None\n",
    "      self.z = None\n",
    "\n",
    "      # Adam optimizer state (initialized to zero)\n",
    "      self.m_weights = np.zeros_like(self.weights)  # 1st moment (mean)\n",
    "      self.v_weights = np.zeros_like(self.weights)  # 2nd moment (variance)\n",
    "      self.m_biases = np.zeros_like(self.biases)\n",
    "      self.v_biases = np.zeros_like(self.biases)\n",
    "      self.t = 0  # Timestep counter (for bias correction)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "      \"\"\"\n",
    "        Compute the output of all neurons in the layer\n",
    "\n",
    "        Args:\n",
    "            inputs: Input data (batch_size, num_inputs)\n",
    "\n",
    "        Returns:\n",
    "            Layer output (batch_size, num_neurons)\n",
    "        \"\"\"\n",
    "      self.input = inputs\n",
    "\n",
    "      # Vectorized computation for all neurons\n",
    "      self.z = np.dot(inputs, self.weights.T) + self.biases\n",
    "\n",
    "      # Apply activation function if provided\n",
    "      if self.activation is not None:\n",
    "         self.output = self.activation(self.z)\n",
    "      else:\n",
    "        self.output = self.z\n",
    "      return self.output\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    Compute gradients w.r.t. weights, biases, and input.\n",
    "\n",
    "    Args:\n",
    "        grad_output: Gradient of loss w.r.t. this layer's OUTPUT (dL/dy)\n",
    "\n",
    "    Returns:\n",
    "        grad_input: Gradient of loss w.r.t. this layer's INPUT (dL/dx)\n",
    "    \"\"\"\n",
    "    # Activation derivative: dL/dz = dL/dy * dy/dz\n",
    "    grad_z = grad_output * self.activation_derivative(self.z)\n",
    "\n",
    "    # Compute gradients for parameters\n",
    "    # dL/dW = (dL/dz)^T @ input  → shape: (num_neurons, num_inputs)\n",
    "    batch_size = self.input.shape[0]\n",
    "    self.grad_weights = np.dot(grad_z.T, self.input) / batch_size\n",
    "    self.grad_biases = np.mean(grad_z, axis=0)\n",
    "\n",
    "    # Gradient w.r.t. input for previous layer: dL/dx = dL/dz @ W\n",
    "    grad_input = np.dot(grad_z, self.weights)\n",
    "\n",
    "    return grad_input\n",
    "\n",
    "  def update_params_adam(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam optimizer.\n",
    "    \"\"\"\n",
    "    self.t += 1  # Increment timestep\n",
    "    \n",
    "    # Update biased first moment estimate (momentum)\n",
    "    self.m_weights = beta1 * self.m_weights + (1 - beta1) * self.grad_weights\n",
    "    self.m_biases = beta1 * self.m_biases + (1 - beta1) * self.grad_biases\n",
    "    \n",
    "    # Update biased second raw moment estimate (velocity)\n",
    "    self.v_weights = beta2 * self.v_weights + (1 - beta2) * (self.grad_weights ** 2)\n",
    "    self.v_biases = beta2 * self.v_biases + (1 - beta2) * (self.grad_biases ** 2)\n",
    "    \n",
    "    # Compute bias-corrected estimates\n",
    "    m_weights_corr = self.m_weights / (1 - beta1 ** self.t)\n",
    "    m_biases_corr = self.m_biases / (1 - beta1 ** self.t)\n",
    "    v_weights_corr = self.v_weights / (1 - beta2 ** self.t)\n",
    "    v_biases_corr = self.v_biases / (1 - beta2 ** self.t)\n",
    "    \n",
    "    # Update parameters\n",
    "    self.weights -= learning_rate * m_weights_corr / (np.sqrt(v_weights_corr) + epsilon)\n",
    "    self.biases -= learning_rate * m_biases_corr / (np.sqrt(v_biases_corr) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85cd6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  def __init__(self, layers, loss_function=None):\n",
    "    self.layers = layers\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "        Forward pass through all layers.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input data (batch_size, num_inputs)\n",
    "\n",
    "        Returns:\n",
    "            Final output after passing through all layers\n",
    "        \"\"\"\n",
    "\n",
    "    for layer in self.layers:\n",
    "      inputs = layer.forward(inputs)\n",
    "    return inputs\n",
    "\n",
    "  def loss(self, y_pred, y_true):\n",
    "    if self.loss_function is None:\n",
    "      raise ValueError(\"No loss function provided\")\n",
    "\n",
    "    loss = self.loss_function(y_pred, y_true)\n",
    "    return loss\n",
    "\n",
    "  def compute_loss_and_grad(self, y_pred, y_true):\n",
    "    \"\"\"Returns (loss_value, grad_wrt_y_pred)\"\"\"\n",
    "    if self.loss_function == \"mse\":\n",
    "      loss = np.mean((y_pred - y_true) ** 2)\n",
    "      grad = 2 * (y_pred - y_true) / y_pred.size # dL/dy_pred\n",
    "      return loss, grad\n",
    "\n",
    "    elif self.loss_function == \"binary_crossentropy\":\n",
    "        # Clip predictions to prevent log(0) and division by zero\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        loss = -np.mean(\n",
    "            y_true * np.log(y_pred_clipped) + \n",
    "            (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "        )\n",
    "        \n",
    "        grad = (\n",
    "            -(y_true / y_pred_clipped) + \n",
    "            (1 - y_true) / (1 - y_pred_clipped)\n",
    "        ) / y_pred.shape[0]  # Divide by batch size for mean\n",
    "        return loss, grad\n",
    "\n",
    "    elif self.loss_function == \"categorical_crossentropy\":\n",
    "      # Compute softmax probabilities\n",
    "      exp_logits = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
    "      softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "      \n",
    "      # Clip probabilities to prevent log(0)\n",
    "      softmax_probs = np.clip(softmax_probs, 1e-7, 1 - 1e-7)\n",
    "      \n",
    "      # Compute loss\n",
    "      loss = -np.mean(np.sum(y_true * np.log(softmax_probs), axis=1))\n",
    "      \n",
    "      # Gradient for cross-entropy with softmax: dL/dz = softmax(z) - y_true\n",
    "      grad = (softmax_probs - y_true) / y_true.shape[0]\n",
    "      \n",
    "      return loss, grad\n",
    "\n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported loss function: {self.loss_function}\")\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    \"\"\"Compute gradients for all layers\"\"\"\n",
    "    # Get gradient from loss\n",
    "    loss, grad_output = self.compute_loss_and_grad(y_pred, y_true)\n",
    "\n",
    "    # Step 2: Backpropagate through layers in REVERSE order\n",
    "    for layer in reversed(self.layers):\n",
    "      grad_output = layer.backward(grad_output)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def update_params_adam(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"Update all layers using Adam optimizer\"\"\"\n",
    "    for layer in self.layers:\n",
    "      layer.update_params_adam(learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "  def train_step(self, X, y, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"Single training step with Adam\"\"\"\n",
    "    y_pred = self.forward(X)\n",
    "    loss = self.backward(y_pred, y)\n",
    "    self.update_params_adam(learning_rate, beta1, beta2, epsilon)\n",
    "    return loss\n",
    "\n",
    "  def train(self, X, y, epochs=100, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, batch_size=None):\n",
    "    \"\"\"\n",
    "    Train the network using Adam optimizer.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (n_samples, n_features)\n",
    "        y: Target labels (n_samples, n_outputs)\n",
    "        epochs: Number of passes through the dataset\n",
    "        learning_rate: Adam learning rate (default 0.001)\n",
    "        batch_size: If None, use full batch\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    batch_size = batch_size or n_samples\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      epoch_loss = 0.0\n",
    "      # Shuffle data for each epoch\n",
    "      indices = np.random.permutation(n_samples)\n",
    "      X_shuffled = X[indices]\n",
    "      y_shuffled = y[indices]\n",
    "\n",
    "      for i in range(0, n_samples, batch_size):\n",
    "        batch_X = X_shuffled[i:i+batch_size]\n",
    "        batch_y = y_shuffled[i:i+batch_size]\n",
    "\n",
    "        loss = self.train_step(batch_X, batch_y, learning_rate, beta1, beta2, epsilon)\n",
    "        epoch_loss += loss * len(batch_X)  # Weight by batch size\n",
    "\n",
    "      avg_loss = epoch_loss / n_samples\n",
    "      if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041140b7",
   "metadata": {},
   "source": [
    "# XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d39ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "layers = [\n",
    "    Layer(8, 2, activation='tanh'),\n",
    "    Layer(1, 8, activation='sigmoid')\n",
    "]\n",
    "net = NeuralNetwork(layers, loss_function=\"binary_crossentropy\")\n",
    "\n",
    "net.train(X, y, epochs=500, learning_rate=0.01, batch_size=4)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "preds = net.forward(X)\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]} → {preds[i][0]:.4f} (target: {y[i][0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c5a44",
   "metadata": {},
   "source": [
    "# Training on MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb66a12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding (for cross-entropy loss)\n",
    "def to_one_hot(y, num_classes=10):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = to_one_hot(y_train)\n",
    "y_test_onehot = to_one_hot(y_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe844c03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Compute accuracy for classification\"\"\"\n",
    "    if y_pred.shape[1] > 1:  # Multi-class\n",
    "        pred_classes = np.argmax(y_pred, axis=1)\n",
    "        true_classes = np.argmax(y_true, axis=1)\n",
    "    else:  # Binary\n",
    "        pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "        true_classes = y_true.flatten()\n",
    "    return np.mean(pred_classes == true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23176ac5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build network: 784 → 128 → 64 → 10\n",
    "layers = [\n",
    "    Layer(128, 784, activation='relu'),\n",
    "    Layer(64, 128, activation='relu'),\n",
    "    Layer(10, 64, activation='linear')  # Output layer: logits for cross-entropy\n",
    "]\n",
    "\n",
    "net = NeuralNetwork(layers, loss_function=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d8cd3",
   "metadata": {},
   "source": [
    "# Test MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674f78d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train on subset for faster testing (use full dataset for best results)\n",
    "train_size = 10000  # Reduce for faster training\n",
    "X_train_subset = X_train[:train_size]\n",
    "y_train_subset = y_train_onehot[:train_size]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "net.train(\n",
    "    X_train_subset, \n",
    "    y_train_subset, \n",
    "    epochs=50, \n",
    "    learning_rate=0.001, \n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_preds = net.forward(X_train_subset)\n",
    "train_acc = accuracy(train_preds, y_train_subset)\n",
    "print(f\"\\nTraining Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "test_preds = net.forward(X_test)\n",
    "test_acc = accuracy(test_preds, y_test_onehot)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e6e53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
